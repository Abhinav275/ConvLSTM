{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings():\n",
    "    def __init__(self, vocab, emd_dims):\n",
    "        self.vocab = vocab\n",
    "        self.embeds = nn.Embedding(len(vocab), emd_dims)\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        lookup_tensor = torch.tensor(self.vocab[word], dtype = torch.long)\n",
    "        return self.embeds(lookup_tensor)\n",
    "    \n",
    "    def vocab_size():\n",
    "        return len(self.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/coco/coco_test_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/coco/coco_test_all.csv\n",
      "./data/coco/coco_train_all.csv\n",
      "./data/coco/coco_val_all.csv\n",
      "./data/bing/bing_train_all.csv\n",
      "./data/bing/bing_test_all.csv\n",
      "./data/bing/bing_val_all.csv\n",
      "./data/flickr/flickr_val_all.csv\n",
      "./data/flickr/flickr_train_all.csv\n",
      "./data/flickr/flickr_test_all.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = ['coco','bing','flickr']\n",
    "dataframes = []\n",
    "for f in data_folder:\n",
    "    files = os.listdir('./data/'+f)   \n",
    "    for path in files:\n",
    "        if('.csv' in path):\n",
    "            csv_path = './data/'+f+'/'+path\n",
    "            print(csv_path)\n",
    "            df = pd.read_csv(csv_path)\n",
    "            dataframes.append(df)\n",
    "len(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14815"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat(dataframes, axis = 0)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12098"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = list(df['questions'])\n",
    "freq = {}\n",
    "for q in questions:\n",
    "    for question in q.split('---'):\n",
    "        wordlist = nltk.word_tokenize(question)\n",
    "        for word in wordlist:\n",
    "            if(word not in freq):\n",
    "                freq[word] = 1\n",
    "            else:\n",
    "                freq[word] += 1\n",
    "len(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5062"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {}\n",
    "counter = 0\n",
    "for key in freq.keys():\n",
    "    if freq[key]>=3:\n",
    "        vocab[key] = counter\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "vocab['<eoq>'] = counter\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vqgnet(nn.Module):\n",
    "    def __init__(self, num_lstm_layers, embedding, max_len):\n",
    "        super(Vqgnet, self).__init__()\n",
    "        self.embedding = embedding\n",
    "        self.model_vgg = models.vgg19(pretrained=True)\n",
    "        for p in self.model_vgg.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.features = self.model_vgg.classifier[:-1]\n",
    "        self.transform_layer = nn.Linear(4096, 512)\n",
    "        self.feature_to_word = nn.Linear(512, self.embedding.vocab_size())\n",
    "        self.n_lstm_layers = n_lstm_layers\n",
    "        self.lstm = nn.LSTM(512, 512)\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def forward(image, question):\n",
    "        # teacher forcing using gt question\n",
    "        \n",
    "        # getting image features\n",
    "        x = self.features(image)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.transform_layer(x))\n",
    "        \n",
    "        cell_state = torch.randn(1, 1, 512)\n",
    "        predicted_question = []\n",
    "        # embedding phase\n",
    "        for i in range(len(question)):\n",
    "            if(i == 0):\n",
    "                embed = x\n",
    "            else:\n",
    "                word = question[i]\n",
    "                embed = self.embedding.get_embedding(word)\n",
    "            output, cell_state = self.lstm(embed, cell_state)\n",
    "            output = F.softmax(self.feature_to_word(output))\n",
    "            predicted_question.append(output)\n",
    "        \n",
    "        return predicted_question\n",
    "    \n",
    "    def test(image):\n",
    "        # get image features\n",
    "        x = self.features(image)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.tranform_layer(x))\n",
    "        \n",
    "        cell_state = torch.randn(1, 1, 512)\n",
    "        output = ''\n",
    "        predicted_question = []\n",
    "        # generate question\n",
    "        while output != self.embedding.vocab_size():\n",
    "            x, cell_state = self.lstm(x, cell_state)\n",
    "            output = F.softmax(self.feature_to_word(x))\n",
    "            predicted_question.append(output)\n",
    "            output = torch.argmax(output)\n",
    "        \n",
    "        return predicted_question\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, scheduler, device, embedding, vocab, num_epochs=25):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    \n",
    "    # looping over number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # looping over train validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "        \n",
    "            # looping over phase data \n",
    "            for image, question in dataloader[phase]:\n",
    "\n",
    "                image = image.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    if(phase == 'train'):\n",
    "                        output = model(image, question)\n",
    "                    else:\n",
    "                        output = model.test(image)\n",
    "                        if len(output) < len(question):\n",
    "                            for i in range(len(question) - len(output)):\n",
    "                                output.append(torch.zeros(output[0].shape))\n",
    "\n",
    "                    # getting one_hot encoding\n",
    "                    one_hot = torch.zeros([len(output), embedding.vocab_size()])\n",
    "                    for i in range(len(question)):\n",
    "                        one_hot[i, vocab[question[i]]] = 1.0\n",
    "                        \n",
    "                    # finding the loss\n",
    "                    loss = criterion(output, one_hot)\n",
    "                    \n",
    "                    # back propogating the weights\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                # adding to loss\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            # finding and printing epoch loss\n",
    "            epoch_loss = running_loss / len(dataloader[phase])\n",
    "            print('{} Loss: {:.4f} '.format(\n",
    "                phase, epoch_loss))\n",
    "            \n",
    "            # appending loss to list \n",
    "            if(phase == 'train'):\n",
    "                train_loss.append(epoch_loss)\n",
    "            else:\n",
    "                val_loss.append(epoch_loss)\n",
    "                    \n",
    "    return train_loss, val_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
