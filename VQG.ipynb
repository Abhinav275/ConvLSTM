{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import nltk\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings():\n",
    "    def __init__(self, vocab, emd_dims):\n",
    "        self.vocab = vocab\n",
    "        self.embeds = nn.Embedding(len(vocab), emd_dims)\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        lookup_tensor = torch.tensor(self.vocab[word], dtype = torch.long)\n",
    "        return self.embeds(lookup_tensor)\n",
    "    \n",
    "    def vocab_size():\n",
    "        return len(self.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/coco/coco_test_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/coco/coco_test_all.csv\n",
      "./data/coco/coco_train_all.csv\n",
      "./data/coco/coco_val_all.csv\n",
      "./data/bing/bing_train_all.csv\n",
      "./data/bing/bing_test_all.csv\n",
      "./data/bing/bing_val_all.csv\n",
      "./data/flickr/flickr_val_all.csv\n",
      "./data/flickr/flickr_train_all.csv\n",
      "./data/flickr/flickr_test_all.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = ['coco','bing','flickr']\n",
    "dataframes = []\n",
    "for f in data_folder:\n",
    "    files = os.listdir('./data/'+f)   \n",
    "    for path in files:\n",
    "        if('.csv' in path):\n",
    "            csv_path = './data/'+f+'/'+path\n",
    "            print(csv_path)\n",
    "            df = pd.read_csv(csv_path)\n",
    "            dataframes.append(df)\n",
    "len(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14815"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat(dataframes, axis = 0)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12098"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = list(df['questions'])\n",
    "freq = {}\n",
    "for q in questions:\n",
    "    for question in q.split('---'):\n",
    "        wordlist = nltk.word_tokenize(question)\n",
    "        for word in wordlist:\n",
    "            if(word not in freq):\n",
    "                freq[word] = 1\n",
    "            else:\n",
    "                freq[word] += 1\n",
    "len(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5062"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {}\n",
    "counter = 0\n",
    "for key in freq.keys():\n",
    "    if freq[key]>=3:\n",
    "        vocab[key] = counter\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "vocab['<eoq>'] = counter\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vqgnet(nn.Module):\n",
    "    def __init__(self, num_lstm_layers, embedding, max_len):\n",
    "        super(Vqgnet, self).__init__()\n",
    "        self.embedding = embedding\n",
    "        self.model_vgg = models.vgg19(pretrained=True)\n",
    "        for p in self.model_vgg.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.features = self.model_vgg.classifier[:-1]\n",
    "        self.transform_layer = nn.Linear(4096, 512)\n",
    "        self.feature_to_word = nn.Linear(512, self.embedding.vocab_size())\n",
    "        self.n_lstm_layers = n_lstm_layers\n",
    "        self.lstm = nn.LSTM(512, 512)\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def forward(image, question):\n",
    "        # teacher forcing using gt question\n",
    "        \n",
    "        # getting image features\n",
    "        x = self.features(image)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.transform_layer(x))\n",
    "        \n",
    "        cell_state = torch.randn(1, 1, 512)\n",
    "        predicted_question = []\n",
    "        # embedding phase\n",
    "        for i in range(len(question)):\n",
    "            if(i == 0):\n",
    "                embed = x\n",
    "            else:\n",
    "                word = question[i]\n",
    "                embed = self.embedding.get_embedding(word)\n",
    "            output, cell_state = self.lstm(embed, cell_state)\n",
    "            output = F.softmax(self.feature_to_word(output))\n",
    "            predicted_question.append(output)\n",
    "        \n",
    "        return predicted_question\n",
    "    \n",
    "    def test(image):\n",
    "        # get image features\n",
    "        x = self.features(image)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.tranform_layer(x))\n",
    "        \n",
    "        cell_state = torch.randn(1, 1, 512)\n",
    "        output = ''\n",
    "        predicted_question = []\n",
    "        # generate question\n",
    "        while output != self.embedding.vocab_size():\n",
    "            x, cell_state = self.lstm(x, cell_state)\n",
    "            output = F.softmax(self.feature_to_word(x))\n",
    "            predicted_question.append(output)\n",
    "            output = torch.argmax(output)\n",
    "        \n",
    "        return predicted_question\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, scheduler, device, embedding, vocab, num_epochs=25):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    \n",
    "    # looping over number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # looping over train validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "        \n",
    "            # looping over phase data \n",
    "            for image, question in dataloader[phase]:\n",
    "\n",
    "                image = image.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    if(phase == 'train'):\n",
    "                        output = model(image, question)\n",
    "                    else:\n",
    "                        output = model.test(image)\n",
    "                        if len(output) < len(question):\n",
    "                            for i in range(len(question) - len(output)):\n",
    "                                output.append(torch.zeros(output[0].shape))\n",
    "\n",
    "                    # getting one_hot encoding\n",
    "                    one_hot = torch.zeros([len(output), embedding.vocab_size()])\n",
    "                    for i in range(len(question)):\n",
    "                        one_hot[i, vocab[question[i]]] = 1.0\n",
    "                        \n",
    "                    # finding the loss\n",
    "                    loss = criterion(output, one_hot)\n",
    "                    \n",
    "                    # back propogating the weights\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                # adding to loss\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            # finding and printing epoch loss\n",
    "            epoch_loss = running_loss / len(dataloader[phase])\n",
    "            print('{} Loss: {:.4f} '.format(\n",
    "                phase, epoch_loss))\n",
    "            \n",
    "            # appending loss to list \n",
    "            if(phase == 'train'):\n",
    "                train_loss.append(epoch_loss)\n",
    "            else:\n",
    "                val_loss.append(epoch_loss)\n",
    "                    \n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "def read_data(csv_file, image_folder):\n",
    "    dataloader = []\n",
    "    df = pd.read_csv(csv_file)\n",
    "    files = os.listdir(image_folder)\n",
    "    for index, row in df.iterrows():\n",
    "        file_name = str(row['image_id']) + '.jpg'\n",
    "        question = row['questions'].split('---')[0]\n",
    "        img = Image.open(image_folder+ '/' + file_name).convert('RGB')\n",
    "        img_tensor = transform(img)\n",
    "        question_words = nltk.word_tokenize(question)\n",
    "        question_words.append('<eoq>')\n",
    "        dataloader.append([img_tensor, question_words])\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data('./data/coco/coco_train_all.csv', './data/coco/train_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = read_data('./data/coco/coco_val_all.csv', './data/coco/val_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainImageLoader = torch.utils.data.DataLoader(train, batch_size=8, shuffle=True)\n",
    "ValImageLoader = torch.utils.data.DataLoader(val, batch_size=8, shuffle=True)\n",
    "dataloaders = {'train':TrainImageLoader, 'val':ValImageLoader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'num_lstm_layers', 'embedding', and 'max_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-6d5fde73230e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVqgnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_is_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'num_lstm_layers', 'embedding', and 'max_len'"
     ]
    }
   ],
   "source": [
    "net = Vqgnet()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda_is_available() else \"cpu\")\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
