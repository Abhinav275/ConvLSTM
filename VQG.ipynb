{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import nltk\n",
    "import PIL.Image as Image\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings():\n",
    "    def __init__(self, vocab, emd_dims):\n",
    "        self.vocab = vocab\n",
    "        self.embeds = nn.Embedding(len(vocab), emd_dims)\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        if(word not in vocab.keys()):\n",
    "            word = '<unk>'\n",
    "        lookup_tensor = torch.tensor(self.vocab[word], dtype = torch.long)\n",
    "        embeds = self.embeds(lookup_tensor)\n",
    "        embeds = embeds.unsqueeze(0)\n",
    "        embeds = embeds.unsqueeze(0)\n",
    "#         print(\"Embedding shape \", embeds.shape)\n",
    "        return embeds.to(device)\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/coco/coco_test_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/coco/coco_test_all.csv\n",
      "./data/coco/coco_train_all.csv\n",
      "./data/coco/coco_val_all.csv\n",
      "./data/bing/bing_train_all.csv\n",
      "./data/bing/bing_test_all.csv\n",
      "./data/bing/bing_val_all.csv\n",
      "./data/flickr/flickr_val_all.csv\n",
      "./data/flickr/flickr_train_all.csv\n",
      "./data/flickr/flickr_test_all.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = ['coco','bing','flickr']\n",
    "dataframes = []\n",
    "for f in data_folder:\n",
    "    files = os.listdir('./data/'+f)   \n",
    "    for path in files:\n",
    "        if('.csv' in path):\n",
    "            csv_path = './data/'+f+'/'+path\n",
    "            print(csv_path)\n",
    "            df = pd.read_csv(csv_path)\n",
    "            dataframes.append(df)\n",
    "len(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14815"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat(dataframes, axis = 0)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12098"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = list(df['questions'])\n",
    "freq = {}\n",
    "for q in questions:\n",
    "    for question in q.split('---'):\n",
    "        wordlist = nltk.word_tokenize(question)\n",
    "        for word in wordlist:\n",
    "            if(word not in freq):\n",
    "                freq[word] = 1\n",
    "            else:\n",
    "                freq[word] += 1\n",
    "len(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5065"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {}\n",
    "counter = 0\n",
    "for key in freq.keys():\n",
    "    if freq[key]>=3:\n",
    "        vocab[key] = counter\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "vocab['<eoq>'] = counter\n",
    "vocab['<start>'] = counter+1\n",
    "vocab['<pad>'] = counter+2\n",
    "vocab['<unk>'] = counter+3\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vqgnet(nn.Module):\n",
    "    def __init__(self, n_lstm_layers, embedding, max_len):\n",
    "        super(Vqgnet, self).__init__()\n",
    "        self.embedding = embedding\n",
    "        self.model_vgg = models.vgg19(pretrained=True)\n",
    "        for p in self.model_vgg.parameters():\n",
    "            p.requires_grad = False\n",
    "        classifier = nn.Sequential(*list(self.model_vgg.classifier.children())[:-1])\n",
    "        self.features = self.model_vgg\n",
    "        self.features.classifier = classifier\n",
    "        self.transform_layer = nn.Linear(4096, 512)\n",
    "        self.feature_to_word = nn.Linear(512, self.embedding.vocab_size())\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.n_lstm_layers = n_lstm_layers\n",
    "        self.lstm = nn.LSTM(512, 512, 8)\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def forward(self, image, question):\n",
    "        # teacher forcing using gt question\n",
    "        \n",
    "        # getting image features\n",
    "        x = self.features(image)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.transform_layer(x))\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        cell_state = torch.randn(8, 1, 512).to(device)\n",
    "        hidden_state = torch.randn(8, 1, 512).to(device)\n",
    "        cell_state = (hidden_state, cell_state)\n",
    "        predicted_question = None\n",
    "        # embedding phase\n",
    "        for i in range(self.max_len):\n",
    "            if(i == 0):\n",
    "                embed = x\n",
    "            else:\n",
    "                embed = question[:,i,:]\n",
    "            output, cell_state = self.lstm(embed, cell_state)\n",
    "            output = self.feature_to_word(output)\n",
    "            output = self.softmax(output)\n",
    "            output = output.squeeze(0)\n",
    "            if(i == 0):\n",
    "                predicted_question = output\n",
    "            else:\n",
    "                predicted_question = torch.cat((predicted_question, output))\n",
    "        \n",
    "        return predicted_question.squeeze(1)\n",
    "    \n",
    "    def test(self, image):\n",
    "        # get image features\n",
    "        x = self.features(image)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.transform_layer(x))\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        cell_state = torch.randn(8, 1, 512).to(device)\n",
    "        hidden_state = torch.randn(8, 1, 512).to(device)\n",
    "        cell_state = (hidden_state, cell_state)\n",
    "        output = x\n",
    "        predicted_question = None\n",
    "        # generate question\n",
    "        pred = 0\n",
    "        for i in range(self.max_len):\n",
    "            output, cell_state = self.lstm(output, cell_state)\n",
    "            pred = self.softmax(self.feature_to_word(output))\n",
    "            pred = pred.squeeze(0)\n",
    "            if(i == 0):\n",
    "                predicted_question = pred\n",
    "            else:\n",
    "                predicted_question = torch.cat((predicted_question, pred))\n",
    "            pred = torch.argmax(pred)\n",
    "        \n",
    "        return predicted_question.squeeze(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, scheduler, device, embedding, vocab, num_epochs=25):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    \n",
    "    # looping over number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # looping over train validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "        \n",
    "            # looping over phase data \n",
    "            for image, question in dataloader[phase]:\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    if(phase == 'train'):\n",
    "#                         print(question)\n",
    "                        output = model(image, question)\n",
    "                    else:\n",
    "                        output = model.test(image)\n",
    "                        if len(output) < len(question):\n",
    "                            for i in range(len(question) - len(output)):\n",
    "                                output.append(torch.zeros(output[0].shape))\n",
    "\n",
    "                    # getting one_hot encoding\n",
    "                    labels = torch.zeros([len(output)]).long().to(device)\n",
    "                    for i in range(len(question)):\n",
    "                        if(question[i][0] not in vocab.keys()):\n",
    "                            labels[i] = vocab['<unk>']\n",
    "                        else:\n",
    "                            labels[i] = vocab[question[i][0]]\n",
    "                        \n",
    "                    # finding the loss\n",
    "                    loss = criterion(output, labels)\n",
    "                    # back propogating the weights\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                # adding to loss\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            # finding and printing epoch loss\n",
    "            epoch_loss = running_loss / len(dataloader[phase])\n",
    "            print('{} Loss: {:.4f} '.format(\n",
    "                phase, epoch_loss))\n",
    "            \n",
    "            # appending loss to list \n",
    "            if(phase == 'train'):\n",
    "                train_loss.append(epoch_loss)\n",
    "            else:\n",
    "                val_loss.append(epoch_loss)\n",
    "                    \n",
    "    return model, train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "def read_data(csv_file, image_folder, embeddings):\n",
    "    dataloader = []\n",
    "    df = pd.read_csv(csv_file)\n",
    "    files = os.listdir(image_folder)\n",
    "    for index, row in df.iterrows():\n",
    "        file_name = str(row['image_id']) + '.jpg'\n",
    "        question = row['questions'].split('---')[0]\n",
    "        img = Image.open(image_folder+ '/' + file_name).convert('RGB')\n",
    "        img_tensor = transform(img)\n",
    "        question_words = nltk.word_tokenize(question)\n",
    "        question_words.append('<eoq>')\n",
    "        question_words = ['<start>'] + question_words\n",
    "        for i in range(len(question_words),26):\n",
    "            question_words.append('<pad>')\n",
    "        question_embeddings = torch.empty(0,1, 512).to(device)\n",
    "        for word in question_words:\n",
    "            question_embeddings = torch.cat((question_embeddings, embeddings.get_embedding(word)))\n",
    "        dataloader.append([img_tensor.to(device), question_embeddings])\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = Embeddings(vocab, 512)\n",
    "train_set = read_data('./data/coco/coco_train_all.csv', './data/coco/train_images', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = read_data('./data/coco/coco_val_all.csv', './data/coco/val_images', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainImageLoader = torch.utils.data.DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "ValImageLoader = torch.utils.data.DataLoader(val_set, batch_size=8, shuffle=True)\n",
    "dataloaders = {'train':TrainImageLoader, 'val':ValImageLoader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Vqgnet(1, embeddings, 26)\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 8.5301 \n",
      "val Loss: 8.5301 \n",
      "Epoch 1/9\n",
      "----------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-61f638cd5ea6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m net, train_loss, val_loss = train_model(net, dataloaders, criterion, optimizer, exp_lr_scheduler,\n\u001b[0;32m----> 2\u001b[0;31m                                         device, embeddings, vocab, num_epochs=10)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-84799328cdda>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, criterion, optimizer, scheduler, device, embedding, vocab, num_epochs)\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0;31m# back propogating the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "net, train_loss, val_loss = train_model(net, dataloaders, criterion, optimizer, exp_lr_scheduler,\n",
    "                                        device, embeddings, vocab, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
